{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.pt.examples import sentences \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('rslp')\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amigos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amigas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amizade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carreira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>carreiras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tempestade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cavaleira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>irmão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>irmã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>carro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>carroça</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_fp\n",
       "0       amigos\n",
       "1       amigas\n",
       "2      amizade\n",
       "3     carreira\n",
       "4    carreiras\n",
       "5   tempestade\n",
       "6    cavaleira\n",
       "7        irmão\n",
       "8         irmã\n",
       "9        carro\n",
       "10     carroça"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_palavras_input_fp = pd.DataFrame(['amigo', 'amizade', 'estudante', 'esquema','carnaval','carnavalesco', 'cavaleira', 'carreiras', 'tempestade'], columns=['original_fp'])\n",
    "df_palavras_input_fp = pd.DataFrame(['amigos', 'amigas', 'amizade', 'carreira', 'carreiras', 'tempestade','cavaleira', 'irmão', 'irmã', 'carro', 'carroça'], columns=['original_fp'])\n",
    "df_palavras_input_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_fp</th>\n",
       "      <th>nltk_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amigos</td>\n",
       "      <td>amig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amigas</td>\n",
       "      <td>amig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amizade</td>\n",
       "      <td>amizad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carreira</td>\n",
       "      <td>carr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>carreiras</td>\n",
       "      <td>carr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tempestade</td>\n",
       "      <td>tempestad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cavaleira</td>\n",
       "      <td>caval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>irmão</td>\n",
       "      <td>irm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>irmã</td>\n",
       "      <td>irmã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>carro</td>\n",
       "      <td>carr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>carroça</td>\n",
       "      <td>carroç</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_fp nltk_stemmer\n",
       "0       amigos         amig\n",
       "1       amigas         amig\n",
       "2      amizade       amizad\n",
       "3     carreira         carr\n",
       "4    carreiras         carr\n",
       "5   tempestade    tempestad\n",
       "6    cavaleira        caval\n",
       "7        irmão          irm\n",
       "8         irmã         irmã\n",
       "9        carro         carr\n",
       "10     carroça       carroç"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_palavras_input_fp['nltk_stemmer'] = [stemmer.stem(palavra) for palavra in df_palavras_input_fp['original_fp']]\n",
    "df_palavras_input_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (934523710.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_palavras_input_fp['spacy_lemma'] = [token.lemma_ for token in doc if token.pos_ == 'NOUN' else token.lemma_]\u001b[0m\n\u001b[0m                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(str([palavra for palavra in df_palavras_input_fp['original_fp']]))\n",
    "\n",
    "df_palavras_input_fp['spacy_lemma'] = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "df_palavras_input_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Eu quero ter aulas na faculdade aprender e ter aprendizado enquanto posso comprar um carro caro\"\n",
    "sent2 = \"Eu quero ir para aula com o carro do meu caro amigo e ver a aula aprender conteúdos felizes que promovam minha aprendizagem\"\n",
    "tokens1 = [t.lower() for t in sent1.split(\" \")]\n",
    "tokens2 = [t.lower() for t in sent2.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eu',\n",
       " 'quero',\n",
       " 'ter',\n",
       " 'aulas',\n",
       " 'na',\n",
       " 'faculdade',\n",
       " 'aprender',\n",
       " 'e',\n",
       " 'ter',\n",
       " 'aprendizado',\n",
       " 'enquanto',\n",
       " 'posso',\n",
       " 'comprar',\n",
       " 'um',\n",
       " 'carro',\n",
       " 'caro']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eu',\n",
       " 'quero',\n",
       " 'ir',\n",
       " 'para',\n",
       " 'aula',\n",
       " 'com',\n",
       " 'o',\n",
       " 'carro',\n",
       " 'do',\n",
       " 'meu',\n",
       " 'caro',\n",
       " 'amigo',\n",
       " 'e',\n",
       " 'ver',\n",
       " 'a',\n",
       " 'aula',\n",
       " 'aprender',\n",
       " 'conteúdos',\n",
       " 'felizes',\n",
       " 'que',\n",
       " 'promovam',\n",
       " 'minha',\n",
       " 'aprendizagem']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aprender', 'carro', 'caro', 'eu', 'e', 'quero'}\n",
      "{'para', 'aula', 'aprender', 'aprendizado', 'promovam', 'que', 'eu', 'e', 'faculdade', 'aprendizagem', 'amigo', 'meu', 'o', 'minha', 'felizes', 'comprar', 'posso', 'ver', 'aulas', 'caro', 'ir', 'enquanto', 'com', 'um', 'na', 'ter', 'conteúdos', 'carro', 'do', 'a', 'quero'}\n"
     ]
    }
   ],
   "source": [
    "set1 = set(tokens1)\n",
    "set2 = set(tokens2)\n",
    "set_union = set1.union(set2)\n",
    "set_intersection = set1.intersection(set2)\n",
    "print(set_intersection)\n",
    "print(set_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple está querendo comprar uma startup do Reino Unido por 100 milhões de dólares\n",
      "Apple PROPN nsubj\n",
      "está AUX aux\n",
      "querendo VERB ROOT\n",
      "comprar VERB xcomp\n",
      "uma DET det\n",
      "startup NOUN obj\n",
      "do ADP case\n",
      "Reino PROPN nmod\n",
      "Unido PROPN flat:name\n",
      "por ADP case\n",
      "100 NUM obl\n",
      "milhões NUM flat\n",
      "de ADP case\n",
      "dólares NOUN nmod\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed sent1: ['eu', 'gostar', 'de', 'estudar']\n",
      "preprocessed sent2: ['eu', 'querer', 'ir', 'para', 'aula', 'com', 'carro', 'de o', 'caro', 'amigo', 'e', 'ver', 'aula', 'aprender', 'conteúdo', 'feliz', 'que', 'promor', 'aprendizagem']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence):\n",
    "    result = []\n",
    "    tokens = nlp(sentence)\n",
    "    for token in tokens:\n",
    "        # remove determiners and punctuation or spaces \n",
    "        if token.pos_ in ['PUNCT','SPACE', 'DET']:\n",
    "            continue\n",
    "        else:\n",
    "            # the lemma of pronouns is stored as '-PRON-' in spacy; we want the actual pronoun\n",
    "            if token.pos_ == 'PRON':\n",
    "                result.append(token.text.lower())\n",
    "            else:\n",
    "                result.append(token.lemma_)\n",
    "    return result\n",
    "\n",
    "print('preprocessed sent1: {}'.format(preprocess(\"Eu gosto de estudar\")))\n",
    "print('preprocessed sent2: {}'.format(preprocess(sent2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_set1 = preprocess(sent1)\n",
    "\n",
    "lemma_set2 = preprocess(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1_lemma = set(lemma_set1)\n",
    "set2_lemma = set(lemma_set2)\n",
    "set_union_lemma = set1.union(set2_lemma)\n",
    "set_intersection_lemma = set1_lemma.intersection(set2_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Coefficient (depois do lemma): 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Jaccard Coefficient (depois do lemma): {}\".format( len(set_intersection_lemma) / len(set_union_lemma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard_coefficient(sent1, sent2, preprocessor=preprocess):\n",
    "#     tokens1 = preprocessor(sent1)\n",
    "#     tokens2 = preprocessor(sent2)\n",
    "#     set1 = set(tokens1)\n",
    "#     set2 = set(tokens2)\n",
    "#     intersection = set1.intersection(set2)\n",
    "#     union = set1.union(set2)\n",
    "#     return len(intersection) / len(union)\n",
    "\n",
    "# print(jaccard_coefficient(sent1,sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respostas = \"\"\"Modelos \n",
    "Utilização\n",
    "Tokenizacao\n",
    "GPT\n",
    "DB como contexto em NLP\n",
    "Análise de sentimento\n",
    "NLP com áudio\n",
    "Tokenization\n",
    "Geração de NL\n",
    "Classificação\n",
    "Como funciona GPT\n",
    "Caso de uso real com NLP\n",
    "Deploy de modelos\n",
    "Arquitetura de redes NLP\n",
    "Bibliotecas\n",
    "Código\n",
    "Girias\n",
    "Utilizar áudio para inter\n",
    "Manter conversa entre IA\n",
    "Técnicas\n",
    "Tratamentos\n",
    "Infraestrutura\n",
    "Arquitetura\n",
    "Qualidade de metadados\n",
    "Qualidade de conceitos de\n",
    "Calculo por trás do nlp\n",
    "Bag of words\n",
    "Aprofundamento em NLP\n",
    "Saudável\n",
    "Aprender em um ritmob\n",
    "Salles\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50cb89ca92bc5f82fc3ea3b5eef052c9ec2eafb18ec3c2a59e48817710300255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
